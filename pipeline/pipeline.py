"""Fetch macroeconomic data from external sources and provide it to user 
   in a clean, friendly way.

This document mimics the data pipline from parsers to end-user API:
    
   1. parsers make data from different sources available
   2. database stores the data
   3. standard (REST) API returns query results for full-syntax query
   4. custom end-user API returns json readable by pd.read_json(),
      the query syntax is simplified for economists to understand
      
You can also see it from the user end:
   - the user wants an economic time series     
   - she queries the api like 
   
   df = pd.read_json('http://mini-kep.herokuapp.com/ru/series/CPI/m/rog/2015')
   
   # that is deliever time series for Russian (ru) consumer price index (CPI)
   # at monthly frequency (m) measured as rate of growth to previous period 
   # (rog) starting from year (2015)
   
   - the query goes to flask app that decodes this into more standard 
   select operation (a GET in REST API call to the database)
   
   - the database should contain such data
   
   - in the database the data was loaded from the parsers which read 
     the original data sources (statuc files and other APIs). 
   
   
   Questions, comments, concerns highly appreciated.
   
   So far:
       > what is implemented and what is not? - added comments along the lines, 
         but we more or less have the parsers and end-user API, no database 
         layer yet (there are prototypes but they are not workable yet)
       > can we live without REST API around the database (#3) - yes we can, 
         it is optional, but I think it is good abstration that can help build 
         other services around the db
       > can you go without jargon like REST and 'reddit'? - I tried and 
         edited ;)    
          
"""


import json
import pandas as pd

# 1. The parsers are ready to provide data
# this dataset is generated by Dataset.yield_dicts() 
# at <https://github.com/mini-kep/parsers/blob/master/parsers/runner.py>

with open('dataset.json') as f:
    data = json.load(f)

# available variable names 
names = set([d['name'] for d in data])


# 2. the database accepted and stores the data 
#    in whichever is most appropriate format 

# what we have have for the database:
    
#    here we tried CRUD methods in pure SQL Alchemy:
#    <https://github.com/mini-kep/db/blob/master/clientdb.py>
#    that kind of hanged here:
#    <https://github.com/mini-kep/db/issues/3>

#    also there is Django app with a simple db schema inside
#    <https://github.com/mini-kep/full-app>
#    but it all stopped here:
#    <https://github.com/mini-kep/full-app/issues/28>
#    if you knwo Djngo well, youcan probablu speed up the process

# code below mimicks a 'select' operation performed by Django ORM or SQLAlchemy

def is_wanted(datapoint, user_query):
    """Does the datapoint comply with user query?"""
    positive_conditions = []
    positive_conditions.append(datapoint['name']==user_query['name'])
    positive_conditions.append(datapoint['freq']==user_query['freq'])
    positive_conditions.append(datapoint['date']>=user_query['start'])
    #skipping this to simplify
    #positive_conditions.append(datapoint['date']<=user_query['end'])
    return all(positive_conditions)


def select(user_query_rest):
    # using global *data*
    return [d for d in data if is_wanted(d, user_query_rest)]


# 3. 'Rest' API for the database
#    The database should accept/send data in json for read/write operations 

# the user sends a query to the database
user_query_rest_sample = dict(name='CPI_rog',
                  freq='m',
                  start='2014-01-01', 
                  end='2015-12-31')

# the server replies
rest_reply = select(user_query_rest_sample)


# 4. Custom API 
# - uses custom, 'reddit-like' notation for end-user API call, as in 
#   <http://mini-kep.herokuapp.com/ru/series/CPI/m/rog/2015>
#    flask knows to decipher long URL and retruns a dict with parameters, try it!
#    the next thing flask should do is return real data 
#
# - returns json readable by pd.read_json()  

def to_json(dicts):    
    df = pd.DataFrame(dicts)
    df.date = df.date.apply(pd.to_datetime)
    df = df.pivot(index='date', values='value', columns='name')
    return df.to_json()

def to_string(pd_date):
    return pd_date.date().strftime('%Y-%m-%d')

def interpret_reddit(user_query_custom):
    name, freq, unit, start_year = user_query_custom.split("/")
    return dict(name=f'{name}_{unit}', 
                freq=freq, 
                start=to_string(pd.to_datetime(start_year)),
                end=to_string(pd.to_datetime(0).today())
                )
    
def custom_select(user_query_custom):
    # break down the *user_query_custom* to *user_query_rest*
    user_query_rest = interpret_reddit(user_query_custom)
    # query rest API
    return select(user_query_rest)

def server_response_custom(user_query_custom):
    return to_json(dicts=custom_select(user_query_custom))

if __name__ == "__main__":
    # this part is user scenario, getting some data inside dataframe    
    
    # the user sends a query to the database
    user_query_custom_sample = 'CPI/m/rog/2017'
    
    # the server replies
    user_result = server_response_custom(user_query_custom_sample)
    
    # the user recieves data and makes a dataframe
    user_df = pd.read_json(user_result)
    
    assert user_df.__str__() == \
"""            CPI_rog
2017-01-31    100.6
2017-02-28    100.2
2017-03-31    100.1
2017-04-30    100.3
2017-05-31    100.4
2017-06-30    100.6
2017-07-31    100.1"""
